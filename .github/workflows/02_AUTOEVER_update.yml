import os
from datetime import datetime, timedelta
import json
import time
import re
import gspread
from google.oauth2.service_account import Credentials
import OpenDartReader
import requests
from bs4 import BeautifulSoup
import pandas as pd
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse
import zipfile
import io

# HTML í…Œì´ë¸” íŒŒì„œ ëŒ€ì•ˆ êµ¬í˜„
try:
    from html_table_parser import parser_functions as parser
    HTML_PARSER_AVAILABLE = True
    print("âœ… html_table_parser ë¡œë“œ ì„±ê³µ")
except ImportError:
    try:
        from html_table_parser_python3 import parser_functions as parser
        HTML_PARSER_AVAILABLE = True
        print("âœ… html_table_parser_python3 ë¡œë“œ ì„±ê³µ")
    except ImportError:
        HTML_PARSER_AVAILABLE = False
        print("âš ï¸ HTML íŒŒì„œ íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚´ì¥ íŒŒì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

class DualSystemDartUpdater:
    """XBRL/HTML ì´ì›í™” ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    # HTML ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì‹œíŠ¸ (ê¸°ì¡´ ë°©ì‹, ì£¼ì„ ì œì™¸)
    HTML_TARGET_SHEETS = [
        'I. íšŒì‚¬ì˜ ê°œìš”', 'II. ì‚¬ì—…ì˜ ë‚´ìš©', '1. ì‚¬ì—…ì˜ ê°œìš”', '2. ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤',
        '3. ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„', '4. ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©', '5. ìœ„í—˜ê´€ë¦¬ ë° íŒŒìƒê±°ë˜',
        '6. ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬í™œë™', '7. ê¸°íƒ€ ì°¸ê³  ì‚¬í•­', '1. ìš”ì•½ì¬ë¬´ì •ë³´',
        '6. ë°°ë‹¹ì— ê´€í•œ ì‚¬í•­', '8. ê¸°íƒ€ ì¬ë¬´ì— ê´€í•œ ì‚¬í•­', 'VII. ì£¼ì£¼ì— ê´€í•œ ì‚¬í•­',
        'VIII. ì„ì› ë° ì§ì› ë“±ì— ê´€í•œ ì‚¬í•­', 'X. ëŒ€ì£¼ì£¼ ë“±ê³¼ì˜ ê±°ë˜ë‚´ìš©',
        'XI. ê·¸ ë°–ì— íˆ¬ìì ë³´í˜¸ë¥¼ ìœ„í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­'
    ]
    
    # XBRL ìš°ì„  ì²˜ë¦¬ ëŒ€ìƒ ì‹œíŠ¸ (ì¬ë¬´ì œí‘œ + ì£¼ì„)
    XBRL_TARGET_SHEETS = [
        '2. ì—°ê²°ì¬ë¬´ì œí‘œ',
        '4. ì¬ë¬´ì œí‘œ',
        '3. ì—°ê²°ì¬ë¬´ì œí‘œ ì£¼ì„',
        '5. ì¬ë¬´ì œí‘œ ì£¼ì„'
    ]

    def __init__(self, corp_code, spreadsheet_var_name, company_name):
        """ì´ˆê¸°í™”"""
        self.corp_code = corp_code
        self.company_name = company_name
        self.spreadsheet_var_name = spreadsheet_var_name
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        print("í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
        required_vars = ['DART_API_KEY', 'GOOGLE_CREDENTIALS', spreadsheet_var_name, 
                        'TELEGRAM_BOT_TOKEN', 'TELEGRAM_CHANNEL_ID']
        for var in required_vars:
            if var in os.environ:
                value = os.environ[var]
                if len(value) > 4:
                    if len(value) > 20:
                        masked_value = value[:6] + '...' + value[-4:-2] + '**'
                    else:
                        masked_value = value[:-2] + '**'
                    print(f"âœ… {var}: {masked_value} (ê¸¸ì´: {len(value)})")
                else:
                    print(f"âš ï¸ {var}: ê°’ì´ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(value)})")
            else:
                print(f"âŒ {var}: ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        if spreadsheet_var_name not in os.environ:
            raise ValueError(f"{spreadsheet_var_name} í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        self.credentials = self.get_credentials()
        self.gc = gspread.authorize(self.credentials)
        self.dart = OpenDartReader(os.environ['DART_API_KEY'])
        self.workbook = self.gc.open_by_key(os.environ[spreadsheet_var_name])
        self.telegram_bot_token = os.environ.get('TELEGRAM_BOT_TOKEN')
        self.telegram_channel_id = os.environ.get('TELEGRAM_CHANNEL_ID')
        
        # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
        self.processing_results = {
            'xbrl_success': [],
            'xbrl_failed': [],
            'html_success': [],
            'html_failed': [],
            'total_processed': 0
        }
        
        # XBRL ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        self.xbrl_namespaces = {
            'xbrl': 'http://www.xbrl.org/2003/instance',
            'ifrs': 'http://xbrl.ifrs.org/taxonomy/2021-03-24/ifrs',
            'ifrs-full': 'http://xbrl.ifrs.org/taxonomy/2021-03-24/ifrs-full',
            'dart': 'http://dart.fss.or.kr/xbrl/taxonomy/kr-gaap',
            'link': 'http://www.xbrl.org/2003/linkbase',
            'xlink': 'http://www.w3.org/1999/xlink'
        }

    def get_credentials(self):
        """Google Sheets ì¸ì¦ ì„¤ì •"""
        creds_json = json.loads(os.environ['GOOGLE_CREDENTIALS'])
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]
        return Credentials.from_service_account_info(creds_json, scopes=scopes)

    def update_dart_reports(self):
        """DART ë³´ê³ ì„œ ë°ì´í„° ì—…ë°ì´íŠ¸ (ì´ì›í™” ì‹œìŠ¤í…œ)"""
        start_date, end_date = self.get_recent_dates()
        report_list = self.dart.list(self.corp_code, start_date, end_date, kind='A', final='T')
        
        if not report_list.empty:
            print(f"ğŸ“‹ ë°œê²¬ëœ ë³´ê³ ì„œ: {len(report_list)}ê°œ")
            
            for _, report in report_list.iterrows():
                print(f"\nğŸ“„ ë³´ê³ ì„œ ì²˜ë¦¬ ì‹œì‘: {report['report_nm']} (ì ‘ìˆ˜ë²ˆí˜¸: {report['rcept_no']})")
                self.processing_results['total_processed'] += 1
                
                # 1ë‹¨ê³„: XBRL ì „ìš© ì²˜ë¦¬ ì‹œë„
                xbrl_success = self.process_xbrl_sheets(report['rcept_no'])
                
                # 2ë‹¨ê³„: HTML ì‹œíŠ¸ ì²˜ë¦¬ (í•­ìƒ ì‹¤í–‰)
                html_success = self.process_html_sheets(report['rcept_no'])
                
                # 3ë‹¨ê³„: ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡
                self.record_processing_result(report, xbrl_success, html_success)
                
        else:
            print("ğŸ“­ ìµœê·¼ 3ê°œì›” ë‚´ ìƒˆë¡œìš´ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self.print_processing_summary()

    def process_xbrl_sheets(self, rcept_no):
        """XBRL ì „ìš© ì‹œíŠ¸ ì²˜ë¦¬"""
        print(f"\nğŸ”¬ XBRL ë°©ì‹ ì²˜ë¦¬ ì‹œì‘: {rcept_no}")
        
        try:
            # XBRL ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
            xbrl_content = self.download_xbrl_data(rcept_no)
            parsed_xbrl = self.parse_xbrl_data(xbrl_content)
            
            if not parsed_xbrl.get('financial_data'):
                print("âš ï¸ XBRLì—ì„œ ì¬ë¬´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # XBRL ì „ìš© ì‹œíŠ¸ì— êµ¬ì¡°í™”ëœ ë°ì´í„° ì €ì¥
            structured_data = self.convert_xbrl_to_structured_data(parsed_xbrl)
            self.update_xbrl_dedicated_sheets(structured_data, rcept_no)
            
            # ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡
            for sheet_name in self.XBRL_TARGET_SHEETS:
                self.processing_results['xbrl_success'].append(sheet_name)
            
            print(f"âœ… XBRL ë°©ì‹ ì²˜ë¦¬ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ XBRL ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
            for sheet_name in self.XBRL_TARGET_SHEETS:
                self.processing_results['xbrl_failed'].append(sheet_name)
            return False

    def process_html_sheets(self, rcept_no):
        """HTML ì „ìš© ì‹œíŠ¸ ì²˜ë¦¬"""
        print(f"\nğŸŒ HTML ë°©ì‹ ì²˜ë¦¬ ì‹œì‘: {rcept_no}")
        
        try:
            report_index = self.dart.sub_docs(rcept_no)
            target_docs = report_index[report_index['title'].isin(self.HTML_TARGET_SHEETS)]
            
            print(f"ğŸ“‘ HTML ì²˜ë¦¬ ëŒ€ìƒ ë¬¸ì„œ: {len(target_docs)}ê°œ")
            
            success_count = 0
            for _, doc in target_docs.iterrows():
                try:
                    print(f"ğŸ“„ HTML ë¬¸ì„œ ì²˜ë¦¬: {doc['title']}")
                    self.update_html_worksheet(doc['title'], doc['url'])
                    self.processing_results['html_success'].append(doc['title'])
                    success_count += 1
                    print(f"âœ… HTML ë¬¸ì„œ ì™„ë£Œ: {doc['title']}")
                    
                except Exception as doc_e:
                    print(f"âŒ HTML ë¬¸ì„œ ì‹¤íŒ¨ {doc['title']}: {str(doc_e)}")
                    self.processing_results['html_failed'].append(doc['title'])
                    continue
            
            print(f"âœ… HTML ë°©ì‹ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(target_docs)}ê°œ ì„±ê³µ")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ HTML ë°©ì‹ ì „ì²´ ì‹¤íŒ¨: {str(e)}")
            return False

    def update_xbrl_dedicated_sheets(self, structured_data, rcept_no):
        """XBRL ì „ìš© ì‹œíŠ¸ ì—…ë°ì´íŠ¸"""
        print(f"ğŸ“Š XBRL ì „ìš© ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹œì‘")
        
        # XBRL ì‹œíŠ¸ëª… ì •ì˜ (êµ¬ë¶„ì„ ìœ„í•´ ì ‘ë‘ì‚¬ ì¶”ê°€)
        xbrl_sheets = {
            'XBRL_ì—°ê²°ì¬ë¬´ì œí‘œ': structured_data,
            'XBRL_ì¬ë¬´ì œí‘œ': structured_data,
            'XBRL_ì—°ê²°ì¬ë¬´ì œí‘œ_ì£¼ì„': self.create_notes_data(structured_data, 'ì—°ê²°', rcept_no),
            'XBRL_ì¬ë¬´ì œí‘œ_ì£¼ì„': self.create_notes_data(structured_data, 'ë³„ë„', rcept_no),
            'XBRL_ì²˜ë¦¬í˜„í™©': self.create_xbrl_status_data(structured_data, rcept_no)
        }
        
        for sheet_name, data in xbrl_sheets.items():
            try:
                # ì‹œíŠ¸ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
                try:
                    worksheet = self.workbook.worksheet(sheet_name)
                except gspread.exceptions.WorksheetNotFound:
                    print(f"ğŸ†• ìƒˆ XBRL ì‹œíŠ¸ ìƒì„±: {sheet_name}")
                    worksheet = self.workbook.add_worksheet(sheet_name, 1000, 15)
                    self.setup_xbrl_sheet_header(worksheet, sheet_name)
                
                # ë°ì´í„° ë³€í™˜ ë° ì—…ë°ì´íŠ¸
                if 'XBRL_ì²˜ë¦¬í˜„í™©' in sheet_name:
                    table_data = self.convert_status_to_table(data)
                elif 'ì£¼ì„' in sheet_name:
                    table_data = self.convert_notes_to_table(data)
                else:
                    table_data = self.convert_xbrl_to_table_format(data)
                
                if table_data:
                    # ê¸°ì¡´ ë°ì´í„° ë³´ì¡´í•˜ë©´ì„œ ìƒˆ ë°ì´í„° ì¶”ê°€
                    self.append_xbrl_data(worksheet, table_data, rcept_no)
                    print(f"âœ… XBRL ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {sheet_name}")
                
            except Exception as sheet_e:
                print(f"âŒ XBRL ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ {sheet_name}: {str(sheet_e)}")
                continue

    def setup_xbrl_sheet_header(self, worksheet, sheet_name):
        """XBRL ì‹œíŠ¸ í—¤ë” ì„¤ì •"""
        if 'XBRL_ì²˜ë¦¬í˜„í™©' in sheet_name:
            headers = ['ì²˜ë¦¬ì¼ì‹œ', 'ì ‘ìˆ˜ë²ˆí˜¸', 'ë³´ê³ ì„œìœ í˜•', 'ì²˜ë¦¬ë°©ì‹', 'ë°ì´í„°ìˆ˜', 'ìƒíƒœ', 'ë¹„ê³ ']
        elif 'ì£¼ì„' in sheet_name:
            headers = ['ì²˜ë¦¬ì¼ì‹œ', 'ì ‘ìˆ˜ë²ˆí˜¸', 'êµ¬ë¶„', 'ì£¼ì„ìœ í˜•', 'ë‚´ìš©', 'í…Œì´ë¸”ìˆ˜', 'ë°ì´í„°ì¶œì²˜', 'ë¹„ê³ ']
        else:
            headers = ['ì²˜ë¦¬ì¼ì‹œ', 'ì ‘ìˆ˜ë²ˆí˜¸', 'êµ¬ë¶„', 'í•­ëª©', 'ë‹¹ê¸°', 'ì „ê¸°', 'ì „ì „ê¸°', 'ë‹¨ìœ„', 'ë°ì´í„°ì¶œì²˜', 'ë¹„ê³ ']
        
        worksheet.update('A1:J1', [headers])
        
        # í—¤ë” ìŠ¤íƒ€ì¼ë§ (ë°°ê²½ìƒ‰ ì„¤ì •)
        try:
            worksheet.format('A1:J1', {
                'backgroundColor': {'red': 0.9, 'green': 0.9, 'blue': 1.0},
                'textFormat': {'bold': True}
            })
        except:
            pass  # ìŠ¤íƒ€ì¼ë§ ì‹¤íŒ¨í•´ë„ ì§„í–‰

    def create_notes_data(self, structured_data, statement_type, rcept_no):
        """ì£¼ì„ ë°ì´í„° ìƒì„±"""
        notes_data = {
            'rcept_no': rcept_no,
            'statement_type': statement_type,  # 'ì—°ê²°' ë˜ëŠ” 'ë³„ë„'
            'processed_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'notes_info': {
                'accounting_policies': 'íšŒê³„ì •ì±… ê´€ë ¨ ì£¼ì„',
                'significant_estimates': 'ì¤‘ìš”í•œ íšŒê³„ì¶”ì • ê´€ë ¨ ì£¼ì„',
                'financial_instruments': 'ê¸ˆìœµìƒí’ˆ ê´€ë ¨ ì£¼ì„',
                'risk_management': 'ìœ„í—˜ê´€ë¦¬ ê´€ë ¨ ì£¼ì„'
            },
            'table_count': 0,  # XBRLì—ì„œ ì¶”ì¶œëœ í…Œì´ë¸” ìˆ˜
            'status': f'XBRL {statement_type} ì£¼ì„ ì²˜ë¦¬ ì™„ë£Œ'
        }
        return notes_data

    def convert_notes_to_table(self, notes_data):
        """ì£¼ì„ ë°ì´í„°ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ ë³€í™˜"""
        table_data = []
        
        for note_type, content in notes_data['notes_info'].items():
            table_data.append([
                notes_data['statement_type'],
                note_type,
                content,
                str(notes_data['table_count']),
                'XBRL',
                f"{notes_data['statement_type']} ì¬ë¬´ì œí‘œ ì£¼ì„"
            ])
        
        return table_data

    def append_xbrl_data(self, worksheet, table_data, rcept_no):
        """XBRL ë°ì´í„°ë¥¼ ê¸°ì¡´ ì‹œíŠ¸ì— ì¶”ê°€"""
        try:
            # í˜„ì¬ ë°ì´í„° í™•ì¸
            existing_data = worksheet.get_all_values()
            
            # ì¤‘ë³µ ë°ì´í„° í™•ì¸ (ê°™ì€ ì ‘ìˆ˜ë²ˆí˜¸)
            duplicate_rows = []
            for i, row in enumerate(existing_data[1:], 2):  # í—¤ë” ì œì™¸
                if len(row) > 1 and row[1] == rcept_no:  # ì ‘ìˆ˜ë²ˆí˜¸ ì»¬ëŸ¼
                    duplicate_rows.append(i)
            
            if duplicate_rows:
                print(f"âš ï¸ ì¤‘ë³µ ë°ì´í„° ë°œê²¬: {len(duplicate_rows)}í–‰. ì‚­ì œ í›„ ì—…ë°ì´íŠ¸")
                # ì¤‘ë³µ í–‰ ì‚­ì œ (ì—­ìˆœìœ¼ë¡œ)
                for row_num in reversed(duplicate_rows):
                    worksheet.delete_rows(row_num)
            
            # ìƒˆ ë°ì´í„° ì¶”ê°€
            if table_data:
                # ê° í–‰ì— ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
                processed_data = []
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                for row in table_data:
                    new_row = [current_time, rcept_no] + row
                    processed_data.append(new_row)
                
                worksheet.append_rows(processed_data)
                print(f"ğŸ“ {len(processed_data)}í–‰ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ XBRL ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            raise

    def create_xbrl_status_data(self, structured_data, rcept_no):
        """XBRL ì²˜ë¦¬ í˜„í™© ë°ì´í„° ìƒì„±"""
        status_data = {
            'rcept_no': rcept_no,
            'processed_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_counts': {
                'balance_sheet': len(structured_data.get('balance_sheet', {})),
                'income_statement': len(structured_data.get('income_statement', {})),
                'cash_flow': len(structured_data.get('cash_flow', {}))
            },
            'company_info': structured_data.get('company_info', {}),
            'status': 'XBRL ì²˜ë¦¬ ì™„ë£Œ'
        }
        return status_data

    def convert_status_to_table(self, status_data):
        """ì²˜ë¦¬ í˜„í™©ì„ í…Œì´ë¸” í˜•íƒœë¡œ ë³€í™˜"""
        table_data = []
        
        total_count = sum(status_data['data_counts'].values())
        
        table_data.append([
            status_data['processed_time'],
            status_data['rcept_no'],
            'ë¶„ê¸°ë³´ê³ ì„œ',
            'XBRL',
            str(total_count),
            status_data['status'],
            f"ì¬ë¬´ìƒíƒœí‘œ:{status_data['data_counts']['balance_sheet']}, ì†ìµê³„ì‚°ì„œ:{status_data['data_counts']['income_statement']}"
        ])
        
        return table_data

    def convert_xbrl_to_table_format(self, structured_data):
        """XBRL êµ¬ì¡°í™” ë°ì´í„°ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ ë³€í™˜"""
        table_data = []
        
        # ì¬ë¬´ìƒíƒœí‘œ ë°ì´í„°
        if structured_data.get('balance_sheet'):
            for item, value in structured_data['balance_sheet'].items():
                table_data.append([
                    'ì¬ë¬´ìƒíƒœí‘œ', item, str(value), '', '', 'KRW', 'XBRL', ''
                ])
        
        # ì†ìµê³„ì‚°ì„œ ë°ì´í„°
        if structured_data.get('income_statement'):
            for item, value in structured_data['income_statement'].items():
                table_data.append([
                    'ì†ìµê³„ì‚°ì„œ', item, str(value), '', '', 'KRW', 'XBRL', ''
                ])
        
        # í˜„ê¸ˆíë¦„í‘œ ë°ì´í„°
        if structured_data.get('cash_flow'):
            for item, value in structured_data['cash_flow'].items():
                table_data.append([
                    'í˜„ê¸ˆíë¦„í‘œ', item, str(value), '', '', 'KRW', 'XBRL', ''
                ])
        
        return table_data

    def update_html_worksheet(self, sheet_name, url):
        """HTML ë°©ì‹ ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë°©ì‹)"""
        try:
            # HTML ì‹œíŠ¸ëŠ” ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            try:
                worksheet = self.workbook.worksheet(sheet_name)
            except gspread.exceptions.WorksheetNotFound:
                worksheet = self.workbook.add_worksheet(sheet_name, 1000, 10)
            
            print(f"ğŸŒ HTML ë°ì´í„° ë‹¤ìš´ë¡œë“œ: {url}")
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            if response.status_code == 200:
                print(f"ğŸ“Š HTML ì½˜í…ì¸  ì²˜ë¦¬...")
                self.process_html_content(worksheet, response.text, sheet_name)
                print(f"âœ… HTML ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {sheet_name}")
            else:
                raise Exception(f"HTTP ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ HTML ì›Œí¬ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            raise

    def process_html_content(self, worksheet, html_content, sheet_name):
        """HTML ë‚´ìš© ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            tables = soup.find_all("table")
            
            # ê¸°ì¡´ ë°ì´í„° ë°±ì—… ë° í´ë¦¬ì–´
            worksheet.clear()
            
            # ì‹œíŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metadata = [
                [f'HTML ì²˜ë¦¬ ì‹œíŠ¸: {sheet_name}'],
                [f'ì²˜ë¦¬ ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'],
                [f'ë°ì´í„° ì¶œì²˜: HTML ìŠ¤í¬ë˜í•‘'],
                ['']  # ë¹ˆ í–‰
            ]
            
            all_data = metadata.copy()
            
            print(f"ë°œê²¬ëœ í…Œì´ë¸” ìˆ˜: {len(tables)}")
            
            for i, table in enumerate(tables):
                table_data = self.parse_html_table_robust(table)
                
                if table_data:
                    print(f"í…Œì´ë¸” {i+1}: {len(table_data)}í–‰ ì¶”ì¶œ")
                    all_data.extend(table_data)
                    all_data.append([''])  # í…Œì´ë¸” ê°„ êµ¬ë¶„
            
            if len(all_data) > len(metadata):
                # ë§ˆì§€ë§‰ ë¹ˆ í–‰ ì œê±°
                if all_data and all_data[-1] == ['']:
                    all_data.pop()
                
                print(f"ì „ì²´ {len(all_data)}í–‰ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
                
                # ë°°ì¹˜ ì—…ë¡œë“œ
                BATCH_SIZE = 100
                for i in range(0, len(all_data), BATCH_SIZE):
                    batch = all_data[i:i + BATCH_SIZE]
                    
                    # í–‰ ê¸¸ì´ ì •ê·œí™”
                    max_cols = max(len(row) for row in batch) if batch else 0
                    normalized_batch = []
                    for row in batch:
                        normalized_row = row + [''] * (max_cols - len(row))
                        normalized_batch.append(normalized_row)
                    
                    try:
                        worksheet.append_rows(normalized_batch)
                        print(f"ë°°ì¹˜ ì—…ë¡œë“œ: {i+1}~{min(i+BATCH_SIZE, len(all_data))} í–‰")
                        
                    except gspread.exceptions.APIError as e:
                        if 'Quota exceeded' in str(e):
                            print("API í• ë‹¹ëŸ‰ ì´ˆê³¼. 60ì´ˆ ëŒ€ê¸°...")
                            time.sleep(60)
                            worksheet.append_rows(normalized_batch)
                        else:
                            raise e
            else:
                print("âš ï¸ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ HTML ì½˜í…ì¸  ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise

    def parse_html_table_robust(self, table):
        """ê²¬ê³ í•œ HTML í…Œì´ë¸” íŒŒì‹±"""
        try:
            if HTML_PARSER_AVAILABLE:
                try:
                    return parser.make2d(table)
                except Exception:
                    pass
            
            # ë‚´ì¥ íŒŒì„œ ì‚¬ìš©
            rows = []
            for tr in table.find_all('tr'):
                row = []
                for cell in tr.find_all(['td', 'th']):
                    text = cell.get_text(separator=' ', strip=True)
                    text = re.sub(r'\s+', ' ', text)
                    row.append(text)
                
                if row:
                    rows.append(row)
            
            return rows
            
        except Exception as e:
            print(f"HTML í…Œì´ë¸” íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return []

    def record_processing_result(self, report, xbrl_success, html_success):
        """ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡"""
        result_status = ""
        if xbrl_success and html_success:
            result_status = "âœ… XBRL+HTML ëª¨ë‘ ì„±ê³µ"
        elif xbrl_success:
            result_status = "ğŸ”¬ XBRLë§Œ ì„±ê³µ"
        elif html_success:
            result_status = "ğŸŒ HTMLë§Œ ì„±ê³µ"
        else:
            result_status = "âŒ ëª¨ë‘ ì‹¤íŒ¨"
        
        print(f"ğŸ“‹ ì²˜ë¦¬ ê²°ê³¼: {report['report_nm']} - {result_status}")

    def print_processing_summary(self):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š === ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===")
        
        # ë‚ ì§œ ë²”ìœ„ ì •ë³´ ì¶œë ¥
        if os.environ.get('MANUAL_START_DATE') and os.environ.get('MANUAL_END_DATE'):
            print(f"ğŸ“… ì²˜ë¦¬ ë²”ìœ„: {os.environ.get('MANUAL_START_DATE')} ~ {os.environ.get('MANUAL_END_DATE')} (ì‚¬ìš©ì ì§€ì •)")
        else:
            print(f"ğŸ“… ì²˜ë¦¬ ë²”ìœ„: ìµœê·¼ 3ê°œì›” (ìë™)")
            
        print(f"ì „ì²´ ë³´ê³ ì„œ ìˆ˜: {self.processing_results['total_processed']}")
        print(f"XBRL ì„±ê³µ: {len(self.processing_results['xbrl_success'])}ê°œ")
        print(f"XBRL ì‹¤íŒ¨: {len(self.processing_results['xbrl_failed'])}ê°œ")
        print(f"HTML ì„±ê³µ: {len(self.processing_results['html_success'])}ê°œ")
        print(f"HTML ì‹¤íŒ¨: {len(self.processing_results['html_failed'])}ê°œ")
        
        # í…”ë ˆê·¸ë¨ ìš”ì•½ ë©”ì‹œì§€
        date_info = ""
        if os.environ.get('MANUAL_START_DATE') and os.environ.get('MANUAL_END_DATE'):
            date_info = f"â€¢ ì²˜ë¦¬ ë²”ìœ„: {os.environ.get('MANUAL_START_DATE')} ~ {os.environ.get('MANUAL_END_DATE')} (ì‚¬ìš©ì ì§€ì •)\n"
        else:
            date_info = "â€¢ ì²˜ë¦¬ ë²”ìœ„: ìµœê·¼ 3ê°œì›” (ìë™)\n"
            
        summary_message = (
            f"ğŸ”„ DART ì´ì›í™” ì²˜ë¦¬ ì™„ë£Œ\n\n"
            f"â€¢ ì¢…ëª©: {self.company_name} ({self.corp_code})\n"
            f"â€¢ ì²˜ë¦¬ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"{date_info}"
            f"â€¢ ì „ì²´ ë³´ê³ ì„œ: {self.processing_results['total_processed']}ê°œ\n"
            f"â€¢ XBRL ì„±ê³µ: {len(self.processing_results['xbrl_success'])}ê°œ (ì¬ë¬´ì œí‘œ+ì£¼ì„)\n"
            f"â€¢ HTML ì„±ê³µ: {len(self.processing_results['html_success'])}ê°œ\n"
            f"â€¢ ì´ ì‹œíŠ¸ ìƒì„±: {len(self.processing_results['xbrl_success']) + len(self.processing_results['html_success'])}ê°œ"
        )
        self.send_telegram_message(summary_message)

    # XBRL ê´€ë ¨ ë©”ì†Œë“œë“¤ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    def download_xbrl_data(self, rcept_no):
        """XBRL ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        print(f"XBRL ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘: {rcept_no}")
        
        try:
            download_url = f"https://opendart.fss.or.kr/disclosureinfo/fnltt/dwld/main.do?rcp_no={rcept_no}"
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/zip, application/xml, text/xml, */*',
                'Referer': 'https://opendart.fss.or.kr/'
            })
            
            response = session.get(download_url, timeout=30)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            content_disposition = response.headers.get('content-disposition', '').lower()
            
            if ('application/zip' in content_type or 
                'application/x-zip' in content_type or 
                '.zip' in content_disposition):
                print("ZIP íŒŒì¼ ê°ì§€, ì••ì¶• í•´ì œ ì¤‘...")
                return self.extract_xbrl_from_zip(response.content)
            
            elif ('xml' in content_type or 
                  response.content.strip().startswith(b'<?xml')):
                print("XML íŒŒì¼ ê°ì§€")
                return response.content.decode('utf-8')
            
            else:
                print(f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹: {content_type}")
                raise ValueError("XBRL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"XBRL ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise

    def extract_xbrl_from_zip(self, zip_content):
        """ZIP íŒŒì¼ì—ì„œ XBRL ë°ì´í„° ì¶”ì¶œ"""
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_ref:
                file_list = zip_ref.namelist()
                print(f"ZIP íŒŒì¼ ë‚´ìš©: {file_list}")
                
                xbrl_patterns = [
                    r'.*\.xbrl$',
                    r'.*xbrl.*\.xml$',
                    r'.*_financial.*\.xml$',
                    r'.*\.xml$'
                ]
                
                xbrl_file = None
                for pattern in xbrl_patterns:
                    matching_files = [f for f in file_list if re.match(pattern, f, re.IGNORECASE)]
                    if matching_files:
                        xbrl_file = max(matching_files, key=lambda x: zip_ref.getinfo(x).file_size)
                        print(f"XBRL íŒŒì¼ ì„ íƒ: {xbrl_file}")
                        break
                
                if xbrl_file:
                    with zip_ref.open(xbrl_file) as f:
                        content = f.read()
                        try:
                            return content.decode('utf-8')
                        except UnicodeDecodeError:
                            try:
                                return content.decode('utf-8-sig')
                            except UnicodeDecodeError:
                                return content.decode('euc-kr')
                else:
                    raise ValueError("ZIP íŒŒì¼ì—ì„œ XBRL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
        except Exception as e:
            print(f"ZIP íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise

    def parse_xbrl_data(self, xbrl_content):
        """XBRL XML ë°ì´í„° íŒŒì‹±"""
        try:
            print("XBRL ë°ì´í„° íŒŒì‹± ì‹œì‘...")
            root = ET.fromstring(xbrl_content)
            print("XML íŒŒì‹± ì„±ê³µ")
            
            for prefix, uri in root.attrib.items():
                if prefix.startswith('xmlns:'):
                    ns_prefix = prefix[6:]
                    self.xbrl_namespaces[ns_prefix] = uri
                elif prefix == 'xmlns':
                    self.xbrl_namespaces['default'] = uri
            
            parsed_data = {
                'contexts': self.extract_contexts(root),
                'financial_data': self.extract_financial_data(root),
                'company_info': self.extract_company_info(root),
                'metadata': self.extract_metadata(root)
            }
            
            print("XBRL ë°ì´í„° íŒŒì‹± ì™„ë£Œ")
            return parsed_data
            
        except Exception as e:
            print(f"XBRL ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            raise

    def extract_contexts(self, root):
        """Context ì •ë³´ ì¶”ì¶œ"""
        contexts = {}
        context_elements = root.findall('.//xbrl:context', self.xbrl_namespaces)
        
        for context in context_elements:
            context_id = context.get('id')
            
            period_info = {}
            period = context.find('xbrl:period', self.xbrl_namespaces)
            if period is not None:
                instant = period.find('xbrl:instant', self.xbrl_namespaces)
                start_date = period.find('xbrl:startDate', self.xbrl_namespaces)
                end_date = period.find('xbrl:endDate', self.xbrl_namespaces)
                
                if instant is not None:
                    period_info['type'] = 'instant'
                    period_info['date'] = instant.text
                elif start_date is not None and end_date is not None:
                    period_info['type'] = 'duration'
                    period_info['start_date'] = start_date.text
                    period_info['end_date'] = end_date.text
            
            contexts[context_id] = {'period': period_info}
        
        return contexts

    def extract_financial_data(self, root):
        """ì¬ë¬´ ë°ì´í„° ì¶”ì¶œ"""
        financial_data = {}
        
        key_items = {
            'Assets': ['ifrs-full:Assets', 'dart:Assets'],
            'Liabilities': ['ifrs-full:Liabilities', 'dart:Liabilities'],
            'Equity': ['ifrs-full:Equity', 'dart:Equity'],
            'Revenue': ['ifrs-full:Revenue', 'dart:Revenue'],
            'ProfitLoss': ['ifrs-full:ProfitLoss', 'dart:ProfitLoss']
        }
        
        for item_name, possible_tags in key_items.items():
            for tag in possible_tags:
                elements = root.findall(f'.//{tag}', self.xbrl_namespaces)
                if elements:
                    item_data = []
                    for elem in elements:
                        item_data.append({
                            'value': elem.text,
                            'context_ref': elem.get('contextRef'),
                            'unit_ref': elem.get('unitRef'),
                            'decimals': elem.get('decimals')
                        })
                    financial_data[item_name] = item_data
                    break
        
        return financial_data

    def extract_company_info(self, root):
        """íšŒì‚¬ ì •ë³´ ì¶”ì¶œ"""
        return {}

    def extract_metadata(self, root):
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        return {}

    def convert_xbrl_to_structured_data(self, parsed_xbrl):
        """XBRL ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""
        structured_data = {
            'balance_sheet': {},
            'income_statement': {},
            'cash_flow': {},
            'company_info': parsed_xbrl.get('company_info', {}),
            'reporting_period': None
        }
        
        financial_mapping = {
            'balance_sheet': {
                'Assets': 'ìì‚°ì´ê³„',
                'Liabilities': 'ë¶€ì±„ì´ê³„',
                'Equity': 'ìë³¸ì´ê³„'
            },
            'income_statement': {
                'Revenue': 'ë§¤ì¶œì•¡',
                'ProfitLoss': 'ë‹¹ê¸°ìˆœì´ìµ'
            }
        }
        
        financial_data = parsed_xbrl.get('financial_data', {})
        
        for statement_type, mapping in financial_mapping.items():
            for xbrl_item, korean_name in mapping.items():
                if xbrl_item in financial_data and financial_data[xbrl_item]:
                    item_data = financial_data[xbrl_item][0]
                    value = item_data['value']
                    
                    if value:
                        try:
                            cleaned_value = re.sub(r'[,\s]', '', value)
                            numeric_value = float(cleaned_value)
                            structured_data[statement_type][korean_name] = numeric_value
                        except ValueError:
                            structured_data[statement_type][korean_name] = value
        
        return structured_data

    # ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° ë©”ì†Œë“œë“¤
    def get_recent_dates(self):
        """ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (ìˆ˜ë™ ì„¤ì • ë˜ëŠ” ê¸°ë³¸ 3ê°œì›”)"""
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        manual_start = os.environ.get('MANUAL_START_DATE')
        manual_end = os.environ.get('MANUAL_END_DATE')
        
        if manual_start and manual_end:
            try:
                # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                start_date = datetime.strptime(manual_start, '%Y%m%d')
                end_date = datetime.strptime(manual_end, '%Y%m%d')
                
                # ë‚ ì§œ ë²”ìœ„ ê²€ì¦
                if start_date > end_date:
                    print("âš ï¸ ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë²”ìœ„ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    return self.get_default_date_range()
                
                # ë„ˆë¬´ ê¸´ ê¸°ê°„ ì œí•œ (ìµœëŒ€ 2ë…„)
                if (end_date - start_date).days > 730:
                    print("âš ï¸ ë‚ ì§œ ë²”ìœ„ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ìµœëŒ€ 2ë…„). ê¸°ë³¸ ë²”ìœ„ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    return self.get_default_date_range()
                
                print(f"ğŸ“… ìˆ˜ë™ ì„¤ì • ë‚ ì§œ ë²”ìœ„: {manual_start} ~ {manual_end}")
                print(f"ğŸ“… ê¸°ê°„: {(end_date - start_date).days + 1}ì¼")
                
                return manual_start, manual_end
                
            except ValueError as e:
                print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {str(e)}. ê¸°ë³¸ ë²”ìœ„ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return self.get_default_date_range()
        else:
            return self.get_default_date_range()
    
    def get_default_date_range(self):
        """ê¸°ë³¸ 3ê°œì›” ë‚ ì§œ ë²”ìœ„"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=90)
        date_range = start_date.strftime('%Y%m%d'), end_date.strftime('%Y%m%d')
        print(f"ğŸ“… ê¸°ë³¸ ë‚ ì§œ ë²”ìœ„ (ìµœê·¼ 3ê°œì›”): {date_range[0]} ~ {date_range[1]}")
        return date_range

    def get_column_letter(self, col_num):
        """ìˆ«ìë¥¼ ì—‘ì…€ ì—´ ë¬¸ìë¡œ ë³€í™˜"""
        result = ""
        while col_num > 0:
            col_num, remainder = divmod(col_num - 1, 26)
            result = chr(65 + remainder) + result
        return result

    def send_telegram_message(self, message):
        """í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡"""
        if not self.telegram_bot_token or not self.telegram_channel_id:
            print("ğŸ“± í…”ë ˆê·¸ë¨ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            url = f"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage"
            data = {
                "chat_id": self.telegram_channel_id,
                "text": message,
                "parse_mode": "HTML"
            }
            response = requests.post(url, data=data)
            response.raise_for_status()
            print("ğŸ“± í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
        except Exception as e:
            print(f"ğŸ“± í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {str(e)}")

    def remove_parentheses(self, value):
        """ê´„í˜¸ ë‚´ìš© ì œê±°"""
        if not value:
            return value
        return re.sub(r'\s*\(.*?\)\s*', '', value).replace('%', '')

    def process_archive_data(self, archive, start_row, last_col):
        """ì•„ì¹´ì´ë¸Œ ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            print(f"ğŸ“Š Archive ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: í–‰={start_row}, ì—´={last_col}")
            
            current_cols = archive.col_count
            target_col_letter = self.get_column_letter(last_col)
            
            if last_col >= current_cols:
                new_cols = last_col + 5
                print(f"ğŸ”§ ì‹œíŠ¸ í¬ê¸° ì¡°ì •: {current_cols} â†’ {new_cols}")
                archive.resize(rows=archive.row_count, cols=new_cols)
                time.sleep(2)

            all_rows = archive.get_all_values()
            update_data = []
            sheet_cache = {}
            
            sheet_rows = {}
            processed_count = 0
            
            for row_idx in range(start_row - 1, len(all_rows)):
                if len(all_rows[row_idx]) < 5:
                    continue
                    
                sheet_name = all_rows[row_idx][0]
                if not sheet_name:
                    continue
                
                if sheet_name not in sheet_rows:
                    sheet_rows[sheet_name] = []
                    
                sheet_rows[sheet_name].append({
                    'row_idx': row_idx + 1,
                    'keyword': all_rows[row_idx][1],
                    'n': all_rows[row_idx][2],
                    'x': all_rows[row_idx][3],
                    'y': all_rows[row_idx][4]
                })
                processed_count += 1
            
            print(f"ğŸ“‹ ì²˜ë¦¬í•  ì‹œíŠ¸ ìˆ˜: {len(sheet_rows)}, ì´ í–‰ ìˆ˜: {processed_count}")
            
            for sheet_name, rows in sheet_rows.items():
                try:
                    print(f"\nğŸ” ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ (í•­ëª©: {len(rows)}ê°œ)")
                    
                    if sheet_name not in sheet_cache:
                        try:
                            search_sheet = self.workbook.worksheet(sheet_name)
                            sheet_data = search_sheet.get_all_values()
                            df = pd.DataFrame(sheet_data)
                            sheet_cache[sheet_name] = df
                            print(f"âœ… ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ: {df.shape}")
                        except gspread.exceptions.WorksheetNotFound:
                            print(f"âš ï¸ ì‹œíŠ¸ '{sheet_name}' ì—†ìŒ. ê±´ë„ˆëœ€.")
                            continue
                    
                    df = sheet_cache[sheet_name]
                    
                    for row in rows:
                        try:
                            keyword = row['keyword']
                            if not all([keyword, row['n'], row['x'], row['y']]):
                                continue
                            
                            n, x, y = int(row['n']), int(row['x']), int(row['y'])
                            
                            keyword_positions = []
                            for idx, df_row in df.iterrows():
                                for col_idx, value in enumerate(df_row):
                                    if str(value).strip() == keyword.strip():
                                        keyword_positions.append((idx, col_idx))
                            
                            if keyword_positions and len(keyword_positions) >= n:
                                target_pos = keyword_positions[n - 1]
                                target_row = target_pos[0] + y
                                target_col = target_pos[1] + x
                                
                                if (0 <= target_row < df.shape[0] and 
                                    0 <= target_col < df.shape[1]):
                                    value = df.iat[target_row, target_col]
                                    cleaned_value = self.remove_parentheses(str(value))
                                    update_data.append((row['row_idx'], cleaned_value))
                                    print(f"âœ… ê°’ ë°œê²¬: {keyword} â†’ {cleaned_value}")
                                else:
                                    print(f"âš ï¸ ë²”ìœ„ ì´ˆê³¼: {keyword}")
                            else:
                                print(f"âš ï¸ í‚¤ì›Œë“œ ë¯¸ë°œê²¬: {keyword}")
                                
                        except Exception as row_e:
                            print(f"âŒ í–‰ ì²˜ë¦¬ ì˜¤ë¥˜: {str(row_e)}")
                            continue
                
                except Exception as sheet_e:
                    print(f"âŒ ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì˜¤ë¥˜: {str(sheet_e)}")
                    continue
            
            print(f"\nğŸ“Š ì—…ë°ì´íŠ¸í•  ë°ì´í„°: {len(update_data)}ê°œ")
            
            if update_data:
                self.update_archive_column(archive, update_data, target_col_letter, last_col)
                
                today = datetime.now()
                three_months_ago = today - timedelta(days=90)
                year = str(three_months_ago.year)[2:]
                quarter = (three_months_ago.month - 1) // 3 + 1
                quarter_text = f"{quarter}Q{year}"
                
                meta_updates = [
                    {'range': 'J1', 'values': [[today.strftime('%Y-%m-%d')]]},
                    {'range': f'{target_col_letter}1', 'values': [['1']]},
                    {'range': f'{target_col_letter}5', 'values': [[today.strftime('%Y-%m-%d')]]},
                    {'range': f'{target_col_letter}6', 'values': [[quarter_text]]}
                ]
                
                archive.batch_update(meta_updates)
                print(f"âœ… ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ (ë¶„ê¸°: {quarter_text})")
                
                message = (
                    f"ğŸ”„ DART Archive ì—…ë°ì´íŠ¸ ì™„ë£Œ\n\n"
                    f"â€¢ ì¢…ëª©: {self.company_name} ({self.corp_code})\n"
                    f"â€¢ ë¶„ê¸°: {quarter_text}\n"
                    f"â€¢ ì—…ë°ì´íŠ¸ ì¼ì‹œ: {today.strftime('%Y-%m-%d %H:%M:%S')}\n"
                    f"â€¢ ì²˜ë¦¬ëœ í–‰: {len(update_data)}ê°œ\n"
                    f"â€¢ ì‹œíŠ¸ ì—´: {target_col_letter} (#{last_col})"
                )
                self.send_telegram_message(message)
                
            else:
                print("âš ï¸ ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            error_msg = f"Archive ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(f"âŒ {error_msg}")
            self.send_telegram_message(f"âŒ {error_msg}")
            raise

    def update_archive_column(self, archive, update_data, target_col_letter, last_col):
        """Archive ì—´ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        try:
            min_row = min(row for row, _ in update_data)
            max_row = max(row for row, _ in update_data)
            
            column_data = [''] * (max_row - min_row + 1)
            for row, value in update_data:
                adjusted_row = row - min_row
                column_data[adjusted_row] = value
            
            column_data_2d = [[value] for value in column_data]
            
            range_label = f'{target_col_letter}{min_row}:{target_col_letter}{max_row}'
            print(f"ğŸ“ ì—…ë°ì´íŠ¸ ë²”ìœ„: {range_label}")
            
            archive.batch_update([{
                'range': range_label,
                'values': column_data_2d
            }])
            
            print(f"âœ… ì»¬ëŸ¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {min_row}~{max_row} í–‰")
            
        except Exception as e:
            print(f"âŒ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        import sys
        
        def log(msg):
            print(f"ğŸ¤– {msg}")
            sys.stdout.flush()
        
        COMPANY_INFO = {
            'code': '307950',
            'name': 'í˜„ëŒ€ì˜¤í† ì—ë²„',
            'spreadsheet_var': 'AUTOEVER_SPREADSHEET_ID'
        }
        
        log(f"{COMPANY_INFO['name']}({COMPANY_INFO['code']}) ì´ì›í™” ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì‹œì‘")
        
        # ë‚ ì§œ ë²”ìœ„ ì •ë³´ ë¡œê·¸
        if os.environ.get('MANUAL_START_DATE') and os.environ.get('MANUAL_END_DATE'):
            log(f"ğŸ“… ì‚¬ìš©ì ì§€ì • ë‚ ì§œ ë²”ìœ„: {os.environ.get('MANUAL_START_DATE')} ~ {os.environ.get('MANUAL_END_DATE')}")
        else:
            log(f"ğŸ“… ê¸°ë³¸ ë‚ ì§œ ë²”ìœ„: ìµœê·¼ 3ê°œì›”")
        
        try:
            updater = DualSystemDartUpdater(
                COMPANY_INFO['code'], 
                COMPANY_INFO['spreadsheet_var'],
                COMPANY_INFO['name']
            )
            
            # ì´ì›í™” ì‹œìŠ¤í…œìœ¼ë¡œ ë³´ê³ ì„œ ì—…ë°ì´íŠ¸
            log("ğŸ“‹ ì´ì›í™” DART ë³´ê³ ì„œ ì—…ë°ì´íŠ¸ ì‹œì‘...")
            updater.update_dart_reports()
            log("âœ… ì´ì›í™” DART ë³´ê³ ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
            # Archive ì‹œíŠ¸ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            log("ğŸ“Š Archive ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹œì‘...")
            archive = updater.workbook.worksheet('Dart_Archive')
            
            sheet_values = archive.get_all_values()
            if not sheet_values:
                raise ValueError("Dart_Archive ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            last_col = len(sheet_values[0])
            control_value = archive.cell(1, last_col).value
            start_row = 10
            
            if control_value:
                last_col += 1
            
            log(f"Archive ì²˜ë¦¬: ì‹œì‘í–‰={start_row}, ëŒ€ìƒì—´={last_col}")
            updater.process_archive_data(archive, start_row, last_col)
            log("âœ… Archive ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
            log("ğŸ‰ ì´ì›í™” ì‹œìŠ¤í…œ ì „ì²´ ì‘ì—… ì™„ë£Œ!")
            
        except Exception as e:
            log(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            if 'updater' in locals():
                updater.send_telegram_message(
                    f"âŒ ì´ì›í™” DART ì—…ë°ì´íŠ¸ ì‹¤íŒ¨\n\n"
                    f"â€¢ ì¢…ëª©: {COMPANY_INFO['name']} ({COMPANY_INFO['code']})\n"
                    f"â€¢ ì˜¤ë¥˜: {str(e)}"
                )
            raise

    except Exception as e:
        print(f"ğŸ’¥ ì „ì²´ ì‘ì—… ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        print(f"ğŸ” ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        raise e

if __name__ == "__main__":
    main()
